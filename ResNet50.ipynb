{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966db3c8-2073-4700-95c3-fdd8278d0ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:16:09.421439: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-21 00:16:09.432455: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 00:16:09.503970: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 00:16:09.763592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 00:16:11.660323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed7ff71-c070-45e9-b3cc-20a4b690717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow._api.v2.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de040f5b-712d-413c-bea2-e85b6a635799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras>=3.0.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (1.62.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: optree in /mnt/kperveen/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: rich in /mnt/kperveen/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /mnt/kperveen/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /mnt/kperveen/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /mnt/kperveen/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f1b605-f7c0-4e1b-b489-bf95d289dac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d8eb39-a18e-4158-a869-204a69741605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f639721-b346-435f-b337-40f56b1117fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_train = \"Train/\"\n",
    "base_dir_test = \"Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64e4e66-d92b-4a0f-8640-de99380dfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_name, csv_file, base_dir, label):\n",
    "    df = spark.read.csv(base_dir + csv_file, header=False, inferSchema=True).toDF(\"filename\")\n",
    "    df = df.withColumn(\"path\", F.concat(lit(base_dir + folder_name + \"/\"), col(\"filename\")))\n",
    "    paths = df.select(\"path\").rdd.flatMap(lambda x: x).collect()\n",
    "    if paths:\n",
    "        images_df = spark.read.format('image').load(paths)\n",
    "        images_df = images_df.select(\"image.*\")\n",
    "    else:\n",
    "        images_df = spark.createDataFrame([], schema=None)\n",
    "    images_df = images_df.withColumn(\"label\", F.lit(label))\n",
    "    return images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c59f93-297c-4f24-a340-e0315dee657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:16:28,787 INFO (Thread-5-660439) Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2024-04-21 00:16:28,789 INFO (Thread-5-660439) Closing down clientserver connection\n",
      "2024-04-21 00:16:28,790 INFO (Thread-5-660439) Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "2024-04-21 00:16:28,799 INFO (Thread-5-660439) Closing down clientserver connection\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "covid_images_df_train = load_images(\"COVID19\", \"COVID19_split_Kausar.csv\", base_dir_train, 1)\n",
    "normal_images_df = load_images(\"NORMAL\", \"NORMAL_split_Kausar.csv\", base_dir_train, 0)\n",
    "pneumonia_images_df = load_images(\"PNEUMONIA\", \"PNEUMONIA_split_Kausar.csv\", base_dir_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "736ca85a-6de2-4b71-bad3-27a8f171c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [normal_images_df, covid_images_df_train, pneumonia_images_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041b9e23-8779-4fbd-b161-94c65c10b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = reduce(DataFrame.unionAll, data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17fd689c-c529-4c36-ac04-8caba1eb4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"ResNet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2914e269-f973-4697-8475-440a20b428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=5, regParam=0.03, elasticNetParam=0.5, labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ba21447-7d06-4425-b3c5-6488ac9caf37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sparkdn \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[featurizer, lr])\n\u001b[0;32m----> 2\u001b[0m spark_model \u001b[38;5;241m=\u001b[39m \u001b[43msparkdn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/pipeline.py:132\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n\u001b[1;32m    131\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[0;32m--> 132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sparkdl/transformers/named_image.py:158\u001b[0m, in \u001b[0;36mDeepImageFeaturizer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    155\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m _NamedImageTransformer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetInputCol(),\n\u001b[1;32m    156\u001b[0m                                          outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetOutputCol(),\n\u001b[1;32m    157\u001b[0m                                          modelName\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetModelName(), featurize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sparkdl/transformers/named_image.py:211\u001b[0m, in \u001b[0;36m_NamedImageTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 211\u001b[0m     modelGraphSpec \u001b[38;5;241m=\u001b[39m \u001b[43m_buildTFGraphForName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetModelName\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetFeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     inputCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetInputCol()\n\u001b[1;32m    213\u001b[0m     resizedCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sdl_imagesResized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sparkdl/transformers/named_image.py:229\u001b[0m, in \u001b[0;36m_buildTFGraphForName\u001b[0;34m(name, featurize)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buildTFGraphForName\u001b[39m(name, featurize):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Currently only supports pre-trained models from the Keras applications module.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     modelData \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_apps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetKerasApplicationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetModelData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     sess \u001b[38;5;241m=\u001b[39m modelData[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    231\u001b[0m     outputTensorName \u001b[38;5;241m=\u001b[39m modelData[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputTensorName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sparkdl/transformers/keras_applications.py:42\u001b[0m, in \u001b[0;36mKerasApplicationModel.getModelData\u001b[0;34m(self, featurize)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetModelData\u001b[39m(\u001b[38;5;28mself\u001b[39m, featurize):\n\u001b[0;32m---> 42\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sess\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     44\u001b[0m         K\u001b[38;5;241m.\u001b[39mset_learning_phase(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "sparkdn = Pipeline(stages=[featurizer, lr])\n",
    "spark_model = sparkdn.fit(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851ee2e-ec9d-48ae-9845-2e7a079f1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_images_df_train = load_images(\"COVID19\", \"COVID19_split_Kausar.csv\", base_dir_train)\n",
    "normal_images_df = load_images(\"NORMAL\", \"NORMAL_split_Kausar.csv\", base_dir_train)\n",
    "pneumonia_images_df = load_images(\"PNEUMONIA\", \"PNEUMONIA_split_Kausar.csv\", base_dir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6411db-359d-4bd4-bb94-96fb95ed575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_pnemonia = spark.read.csv(\"Train/PNEUMONIA_split_Kausar.csv\")\n",
    "train_files_normal = spark.read.csv(\"Train/NORMAL_split_Kausar.csv\")\n",
    "train_files_covid = spark.read.csv(\"Train/COVID19_split_Kausar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129adea1-3de3-41c5-ae9d-a073b70a0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_pnemonia = spark.read.csv(\"Test/PNEUMONIA_split_Kausar.csv\")\n",
    "test_files_normal = spark.read.csv(\"Test/NORMAL_split_Kausar.csv\")\n",
    "test_files_covid = spark.read.csv(\"Test/COVID19_split_Kausar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc266f-a815-438e-a827-63b78a29bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
